"""BizAnalytix Tech -- Spam and Ham
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1dHnKtdwJhNxE_RCA1au_4TIuTh1xvWKu

This is a Assignment project by Sajal Tiwari for BizAnalytix Internship through Internshala

# Installing and Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
from os import makedirs, path, remove, rename, rmdir
from tarfile import open as open_tar
from urllib import request, parse
from glob import glob
from os import path
from sklearn.model_selection import StratifiedShuffleSplit
from re import sub
from email import message_from_file
import numpy as np
from collections import defaultdict
import joblib
# %pip install joblib
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline
from collections import defaultdict
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from functools import partial
from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score
from sklearn.model_selection import cross_val_predict
from sklearn.neighbors import KNeighborsClassifier

"""# Fetching data and structuring DIR || Loading Email"""

def download_corpus(data_dir:str='data'):
    
    """
        Function to fetch data and arrange it in proper manner
        The Dir Structure should look like
        
                Data
                |-- downloads
                |   |--20021010_easy_ham.tar.bz2
                |   |--20021010_spam.tar.bz2
                |
                |-- ham
                |-- spam
    """

    base_URL = "https://spamassassin.apache.org"
    corpus_PATH = "old/publiccorpus"

    files = {
        '20021010_easy_ham.tar.bz2': 'ham',
        '20021010_spam.tar.bz2': 'spam',
    }

    downloads_dir = path.join(data_dir, 'downloads')
    ham_dir = path.join(data_dir, 'ham')
    spam_dir = path.join(data_dir, 'spam')

    makedirs(downloads_dir, exist_ok=True)
    makedirs(ham_dir, exist_ok=True)
    makedirs(spam_dir, exist_ok=True)

    for file, spam_or_ham in files.items():
        # download file
        url = parse.urljoin(base_URL, f'{corpus_PATH}/{file}')
        tar_filename = path.join(downloads_dir, file)
        request.urlretrieve(url, tar_filename)
        
        # list e-mails in compressed file
        emails = []
        with open_tar(tar_filename) as tar:
            tar.extractall(path=downloads_dir)
            for tarinfo in tar:
                if len(tarinfo.name.split('/')) > 1:
                    emails.append(tarinfo.name)
        
        # move e-mails to ham or spam dir
        for email in emails:
            directory, filename = email.split('/')
            directory = path.join(downloads_dir, directory)
            rename(path.join(directory, filename),
                   path.join(data_dir, spam_or_ham, filename))
        rmdir(directory)

download_corpus()

ham_dir = path.join('data', 'ham')
spam_dir = path.join('data', 'spam')

print('Total Hams  =======>', len(glob(f'{ham_dir}/*')))  
print('Total Spams =======>', len(glob(f'{spam_dir}/*')))

"""## Loading mail as numpy array"""

class SimpleEmail:
    '''
    Structure the mail into simple string clean format
    '''
    def __init__(self, subject: str, body: str):
        self.subject = subject
        self.body = body

    @property
    def clean(self):
        sanitizer = '[^A-Za-z]+'
        clean = sub(sanitizer, ' ', f'{self.subject} {self.body}')
        clean = clean.lower()
        return sub('\s+', ' ', clean)

    def __str__(self):
        subject = f'subject: {self.subject}'
        body_first_line = self.body.split('\n')[0]
        body = f'body: {body_first_line}...'
        return f'{subject}\n{body}'
    def __repr__(self):
        return self.__str__()

class EmailIterator:
    def __init__(self, directory: str):
        self._files = glob(f'{directory}/*')
        self._pos = 0
    
    def __iter__(self):
        self._pos = -1
        return self
    
    def __next__(self):
        if self._pos < len(self._files) - 1:
            self._pos += 1
            return self.parse_email(self._files[self._pos])
        raise StopIteration()
        
    @staticmethod
    def parse_email(filename: str) -> SimpleEmail:
        with open(filename,
                  encoding='utf-8',
                  errors='replace') as fp:
            message = message_from_file(fp)
        
        subject = None
        for item in message.raw_items():
            if item[0] == 'Subject':
                subject = item[1]
        
        if message.is_multipart():
            body = []
            for b in message.get_payload():
                body.append(str(b))
            body = '\n'.join(body)
        else:
            body = message.get_payload()
        
        return SimpleEmail(subject, body)

"""
Data Preprocessing
Converting all the emails to numpy array 
"""
ham_emails = EmailIterator('data/ham')
spam_emails = EmailIterator('data/spam')
hams = np.array([email.clean for email in ham_emails])
spams = np.array([email.clean for email in spam_emails])

print("Length of Hams is -- ",len(hams))
print("Example of Hams is -- ",hams[1])

print("Length of Spams is -- ",len(spams))
print("Example of Spams is -- ",spams[1])

"""# Data Distribution"""

"""
For Even distribution of data into training and test set 
Stratified Shuffel Split is used for even distribution of data into train and test set

"""
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

emails = np.concatenate((hams, spams))

labels = np.concatenate((np.zeros(hams.size), np.ones(spams.size)))

for train_index, test_index in split.split(emails, labels):
    emails_train, labels_train = \
        emails[train_index], labels[train_index]
    emails_test, labels_test = \
        emails[test_index], labels[test_index]

"""### This will change the email string into sparse vector that indicates the presence or absence of each possible word."""

dictionary = defaultdict(int)
for email in emails_train:
    for word in email.split(' '):
        dictionary[word] += 1

top = 1000
descending_dictionary = sorted(dictionary.items(),
                               key=lambda v: v[1],
                               reverse=True)
dictionary = [
    word for (word, occur) in descending_dictionary
    if len(word) > 1
][:top]

def encode_email(email: SimpleEmail,dictionary_: list,binary: bool = False) -> np.array:
    
    '''Encoding mail to vectors based on occurance and non occurance'''
    encoded = np.zeros(dictionary_.size)
    words = email.split(' ')
    
    for word in words:
        index = np.where(dictionary_ == word)[0]
        if index.size == 1:  # we ignore unknown words
            if binary:
                encoded[index[0]] = 1
            else:
                encoded[index[0]] += 1
    return encoded

dictionary = np.array(dictionary)
_encode_email = partial(encode_email, dictionary_=dictionary)
encoded_train = np.array(list(map(_encode_email, emails_train)))
encoded_test = np.array(list(map(_encode_email, emails_test)))

## Deleting non required variables to prevent RAM crash due to exceeding limits
del ham_emails,spam_emails,hams,spams,emails,labels,dictionary,descending_dictionary

"""# Training Model"""

knn_clf = KNeighborsClassifier()

labels_pred = cross_val_predict(knn_clf,encoded_train,labels_train,cv=5)

print('accuracy:', accuracy_score(labels_train, labels_pred))

print('precision:', precision_score(labels_train, labels_pred))

print('recall:', recall_score(labels_train, labels_pred))

print('f1:', f1_score(labels_train, labels_pred))

"""### Using GridSearchCV to get optimal params"""

params_grid = [{
    'n_neighbors': [2, 5, 10],
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'metric': ['minkowski'],
    'metric_params': [{'p': 2}, {'p': 3}, {'p': 4}]
}]

search = GridSearchCV(knn_clf, params_grid, n_jobs=6,
                      scoring='recall', cv=5, verbose=10)

search.fit(encoded_train, labels_train)

print(" Results from Grid Search " )
print("\n The best estimator across ALL searched params:\n",search.best_estimator_)
print("\n The best score across ALL searched params:\n",search.best_score_)
print("\n The best parameters across ALL searched params:\n",search.best_params_)

labels_pred = cross_val_predict(search.best_estimator_,
                                encoded_train,
                                labels_train,
                                cv=5)
print('Accuracy for best params is ======>>', accuracy_score(labels_train, labels_pred))
print('Precision for best params is =====>>', precision_score(labels_train, labels_pred))
print('Recall for best params is ========>>', recall_score(labels_train, labels_pred))
print('f1 Score for best params is ======>>', f1_score(labels_train, labels_pred))

knn_clf = KNeighborsClassifier(algorithm='auto',p=2,
                               n_neighbors=2,
                               weights='distance')
knn_clf.fit(encoded_train, labels_train)
labels_pred = knn_clf.predict(encoded_test)
print('accuracy:', accuracy_score(labels_test, labels_pred))
# accuracy: 0.982896846606093
print('precision:', precision_score(labels_test, labels_pred))
# precision: 0.9666666666666667
print('recall:', recall_score(labels_test, labels_pred))
# recall: 0.9666666666666667
print('f1:', f1_score(labels_test, labels_pred))
# f1: 0.9666666666666667

"""# Building Pipeline and automating all process of Encoding and Training"""

class MessageEncoder(BaseEstimator, TransformerMixin):
    
    def __init__(self, binary: bool = False, top: int = 1000):
        self.dictionary_ = None
        self.binary = binary
        self.top = top
    def fit(self, X, y=None):
        dictionary = defaultdict(int)
        
        for email in X:
            for word in email.split(' '):
                dictionary[word] += 1
        
        descending_dictionary = sorted(dictionary.items(),
                                       key=lambda v: v[1],
                                       reverse=True)
        
        self.dictionary = np.array([
            word for (word, occur) in descending_dictionary
            if len(word) > 1
        ][:self.top])
        
        return self
    def transform(self, X):
        return np.array(list(map(self.encode_message, X)))
    def encode_message(self, message: str):
        encoded = np.zeros(self.dictionary.size)
        words = message.split(' ')
        for word in words:
            index = np.where(self.dictionary == word)[0]
            if index.size == 1:  # we ignore unknown words
                if self.binary:
                    encoded[index[0]] = 1
                else:
                    encoded[index[0]] += 1
        return encoded

encoder = MessageEncoder()
encoder.fit(emails_train)
encoded_emails_train = encoder.transform(emails_train)

pipeline = Pipeline([
    ('encode_messages', MessageEncoder()),
    ('knn_clf', KNeighborsClassifier()),
])

# Here knn_clf__algorithm = 'auto' and knn_clf__weights = distance are used to train model due to shortage of RAM
# the model can be trained using below params if RAM availablity is High
params_grid = [{
    'encode_messages__binary': [True, False],
    'encode_messages__top': [500, 1000],
    'knn_clf__n_neighbors': [2, 5, 10],
    'knn_clf__weights': ['uniform', 'distance'],
    'knn_clf__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
}]

pipe_search = GridSearchCV(pipeline, params_grid, n_jobs=1,
                           scoring='recall', cv=5, verbose=2)
pipe_search.fit(emails_train, labels_train)

"""# Prediction"""

labels_test_pred = pipe_search.best_estimator_.predict(emails_test)
print(classification_report(labels_test,
                            labels_pred,
                            target_names=['ham', 'spam'],
                            digits=4))

"""# Exporting Trained Model """

joblib.dump(pipe_search, 'model_file_name.pkl')




